# Complete Setup Guide - Spark Kafka Cluster on AWS

## What We Built

A distributed data processing cluster on AWS with:
- **1 Bastion Host** (RHEL-based, t3.small) - Ansible control node in public subnet
- **1 Master Node** (Ubuntu, t3.medium) - Zookeeper, Kafka Broker, Spark Master in private subnet
- **2 Worker Nodes** (Ubuntu, t3.medium) - Spark Workers in private subnet

---

## Step-by-Step Commands Executed

### Phase 1: Initial Assessment

**1. Identified existing AWS resources:**
```bash
# Checked bastion instance details
# Found: Bastion IP 10.0.13.122 in eu-north-1 (Stockholm)
# Found: VPC vpc-0eed7af6a5360f7c8
# Found: Security groups already created
```

**2. Got VPC and Subnet information:**
```bash
# Got metadata from bastion instance
TOKEN=`curl -s -X PUT "http://169.254.169.254/latest/api/token" -H "X-aws-ec2-metadata-token-ttl-seconds: 21600"`
echo "VPC ID: $(curl -s -H "X-aws-ec2-metadata-token: $TOKEN" http://169.254.169.254/latest/meta-data/network/interfaces/macs/$(curl -s -H "X-aws-ec2-metadata-token: $TOKEN" http://169.254.169.254/latest/meta-data/mac)/vpc-id)"
echo "Subnet ID: $(curl -s -H "X-aws-ec2-metadata-token: $TOKEN" http://169.254.169.254/latest/meta-data/network/interfaces/macs/$(curl -s -H "X-aws-ec2-metadata-token: $TOKEN" http://169.254.169.254/latest/meta-data/mac)/subnet-id)"

# Result:
# VPC ID: vpc-0eed7af6a5360f7c8
# Public Subnet ID: subnet-0a0a5d9cf662dd381
# Private Subnet ID: subnet-0968e7f834411717e (from AWS Console)
```

**3. Identified existing security groups:**
```bash
# From AWS Console:
# sg-0ebcf91b3e3abc4c1 - ansible-bastion-sg
# sg-052db11f6e080eef4 - kafka-spark-master-sg
```

**4. Found SSH key:**
```bash
# Key pair name: kafka-spark-keypair
# Private key location: /home/ec2-user/kafka-spark-keypair.pem
chmod 400 /home/ec2-user/kafka-spark-keypair.pem
```

---

### Phase 2: Terraform Setup

**1. Created Terraform configuration files:**
```bash
cd ~/terraform

# Files created:
# - provider.tf          (AWS provider config)
# - variables.tf         (Input variables)
# - vpc.tf              (VPC resources - commented out, using existing)
# - security_groups.tf   (Security groups - using existing + new worker SG)
# - ec2.tf              (EC2 instances - master + workers)
# - outputs.tf          (Output values)
# - terraform.tfvars    (Actual values)
```

**2. Configured terraform.tfvars:**
```bash
cat > ~/terraform/terraform.tfvars << 'EOF'
aws_region   = "eu-north-1"
project_name = "spark-kafka-cluster"

existing_vpc_id            = "vpc-0eed7af6a5360f7c8"
existing_public_subnet_id  = "subnet-0a0a5d9cf662dd381"
existing_private_subnet_id = "subnet-0968e7f834411717e"

existing_bastion_sg_id = "sg-0ebcf91b3e3abc4c1"
existing_master_sg_id  = "sg-052db11f6e080eef4"
existing_worker_sg_id  = ""

ssh_key_name = "kafka-spark-keypair"
your_ip = "0.0.0.0/0"

ami_id = "ami-0705384c0b33c194c"

master_instance_type = "t3.medium"
worker_instance_type = "t3.medium"
worker_count         = 2

private_subnet_cidr = "10.0.128.0/20"
EOF
```

**3. Installed AWS CLI:**
```bash
curl "https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip" -o "awscliv2.zip"
unzip -q awscliv2.zip
sudo ./aws/install
```

**4. Configured AWS credentials:**
```bash
aws configure
# Entered:
# AWS Access Key ID: <YOUR_ACCESS_KEY>
# AWS Secret Access Key: <YOUR_SECRET_KEY>
# Default region: eu-north-1
# Default output format: json
```

**5. Installed Terraform:**
```bash
sudo yum install -y yum-utils
sudo yum-config-manager --add-repo https://rpm.releases.hashicorp.com/AmazonLinux/hashicorp.repo
sudo yum -y install terraform
```

**6. Initialized Terraform:**
```bash
cd ~/terraform
terraform init
```

**7. Planned infrastructure:**
```bash
terraform plan
# Reviewed: Will create 1 master, 2 workers, 1 worker SG, IAM role
```

**8. Applied Terraform configuration:**
```bash
terraform apply -auto-approve
```

**Resources Created:**
- Master Node: i-083657b80007ea86e (10.0.136.17)
- Worker 1: i-0b3d8475f3c803aed (10.0.137.14)
- Worker 2: i-0cbd44cdc9eac8c67 (10.0.128.219)
- Worker Security Group: sg-09ec1280749c7ce01
- IAM Role: spark-kafka-cluster-ec2-s3-role
- IAM Instance Profile: spark-kafka-cluster-ec2-profile

---

### Phase 3: Ansible Configuration

**1. Created Ansible directory structure:**
```bash
cd ~/ansible

# Directory structure:
# ansible/
# â”œâ”€â”€ ansible.cfg
# â”œâ”€â”€ inventories/hosts.ini
# â”œâ”€â”€ group_vars/
# â”‚   â”œâ”€â”€ all.yml
# â”‚   â”œâ”€â”€ master.yml
# â”‚   â””â”€â”€ workers.yml
# â”œâ”€â”€ playbooks/site.yml
# â””â”€â”€ roles/
#     â”œâ”€â”€ common/
#     â”œâ”€â”€ java/
#     â”œâ”€â”€ zookeeper/
#     â”œâ”€â”€ kafka/
#     â””â”€â”€ spark/
```

**2. Generated Ansible inventory from Terraform:**
```bash
cd ~/terraform
terraform output -raw ansible_inventory > ../ansible/inventories/hosts.ini
```

**3. Updated inventory for Ubuntu instances:**
```bash
# Discovered instances were Ubuntu, not Amazon Linux
# Updated ansible_user from ec2-user to ubuntu
```

**4. Fixed ansible.cfg (removed YAML header):**
```bash
# Removed --- from top of ansible.cfg
```

**5. Created role defaults:**
```bash
# Created defaults/main.yml for each role with variables
```

**6. Updated group_vars/all.yml:**
```bash
cat > ~/ansible/group_vars/all.yml << 'EOF'
---
ansible_ssh_private_key_file: /home/ec2-user/kafka-spark-keypair.pem
ansible_python_interpreter: /usr/bin/python3
ansible_user: ubuntu

java_version: "11"

spark_version: "3.5.0"
spark_hadoop_version: "3"
spark_download_url: "https://archive.apache.org/dist/spark/spark-{{ spark_version }}/spark-{{ spark_version }}-bin-hadoop{{ spark_hadoop_version }}.tgz"
spark_home: "/opt/spark"
spark_master_port: 7077
spark_master_webui_port: 8080

kafka_version: "3.6.1"
scala_version: "2.13"
kafka_download_url: "https://archive.apache.org/dist/kafka/{{ kafka_version }}/kafka_{{ scala_version }}-{{ kafka_version }}.tgz"
kafka_home: "/opt/kafka"
kafka_port: 9092

zookeeper_port: 2181
zookeeper_data_dir: "/var/lib/zookeeper"

aws_region: "eu-north-1"
s3_bucket_name: "your-bucket-name"

app_user: "spark"
app_group: "spark"
EOF
```

**7. Added SSH access rules to security groups:**
```bash
# SSH from bastion to master (rule already existed)
/usr/local/bin/aws ec2 authorize-security-group-ingress \
  --group-id sg-052db11f6e080eef4 \
  --protocol tcp --port 22 \
  --source-group sg-0ebcf91b3e3abc4c1

# SSH from bastion to workers (rule already existed)
/usr/local/bin/aws ec2 authorize-security-group-ingress \
  --group-id sg-09ec1280749c7ce01 \
  --protocol tcp --port 22 \
  --source-group sg-0ebcf91b3e3abc4c1
```

**8. Tested connectivity:**
```bash
cd ~/ansible
ansible all -m ping -i inventories/hosts.ini
# Result: All hosts reachable
```

**9. Updated Ansible roles for Ubuntu:**
```bash
# Modified roles to support both Debian (apt) and RedHat (yum)
# Updated common, java, zookeeper, kafka, spark roles
```

**10. Fixed playbook issues:**
```bash
# Added ignore_errors to bastion Ansible installation task
# Created defaults/main.yml in each role with default variables
```

**11. Created NAT Gateway for internet access:**
```bash
# Private subnet instances couldn't reach internet for package updates
# Created NAT Gateway in public subnet

# Allocate Elastic IP
NAT_EIP_ALLOC_ID=$(aws ec2 allocate-address --domain vpc --region eu-north-1 --query 'AllocationId' --output text)
# Result: eipalloc-0a1b2c3d4e5f6g7h8 (example)

# Create NAT Gateway
NAT_GW_ID=$(aws ec2 create-nat-gateway \
  --subnet-id subnet-0a0a5d9cf662dd381 \
  --allocation-id $NAT_EIP_ALLOC_ID \
  --region eu-north-1 \
  --query 'NatGateway.NatGatewayId' --output text)

# Wait for availability
aws ec2 wait nat-gateway-available --nat-gateway-ids $NAT_GW_ID --region eu-north-1

# Add route to NAT Gateway in private route table
PRIVATE_RT_ID=$(aws ec2 describe-route-tables \
  --filters "Name=association.subnet-id,Values=subnet-0968e7f834411717e" \
  --region eu-north-1 \
  --query 'RouteTables[0].RouteTableId' --output text)

aws ec2 create-route \
  --route-table-id $PRIVATE_RT_ID \
  --destination-cidr-block 0.0.0.0/0 \
  --nat-gateway-id $NAT_GW_ID \
  --region eu-north-1
```

**12. Updated security group egress rules:**
```bash
# Added outbound internet access for master and workers

# Master security group
aws ec2 authorize-security-group-egress \
  --group-id sg-052db11f6e080eef4 \
  --ip-permissions IpProtocol=-1,IpRanges='[{CidrIp=0.0.0.0/0}]' \
  --region eu-north-1

# Worker security group
aws ec2 authorize-security-group-egress \
  --group-id sg-09ec1280749c7ce01 \
  --ip-permissions IpProtocol=-1,IpRanges='[{CidrIp=0.0.0.0/0}]' \
  --region eu-north-1
```

**13. Fixed Ansible SSH configuration:**
```bash
# Added private_key_file to ansible.cfg
echo "private_key_file = /home/ec2-user/kafka-spark-keypair.pem" >> ~/ansible/ansible.cfg
```

**14. Verified connectivity and ran playbook:**
```bash
cd ~/ansible

# Test ping to all hosts
ansible all -m ping -i inventories/hosts.ini

# Run deployment playbook
ansible-playbook -i inventories/hosts.ini playbooks/site.yml
```

---

## Current Status

### âœ… Completed
- [x] Terraform infrastructure created
- [x] 3 EC2 instances running (1 master + 2 workers)
- [x] Security groups configured
- [x] IAM roles for S3 access created
- [x] Ansible inventory generated
- [x] SSH connectivity working
- [x] Ansible playbook structure created
- [x] Roles created for all components

### ðŸ”„ In Progress
- [ ] Ansible playbook deployment (running now)

### ðŸ“‹ To Do After Playbook Completes
- [ ] Verify Zookeeper is running on master
- [ ] Verify Kafka is running on master
- [ ] Verify Spark master is running
- [ ] Verify Spark workers are connected
- [ ] Test Kafka topic creation
- [ ] Test Spark job submission
- [ ] Deploy Scala application

---

## Quick Reference Commands

### Check Infrastructure
```bash
# List EC2 instances
/usr/local/bin/aws ec2 describe-instances \
  --filters "Name=tag:Project,Values=spark-kafka-cluster" \
  --query 'Reservations[*].Instances[*].[Tags[?Key==`Name`].Value|[0],InstanceId,State.Name,PrivateIpAddress]' \
  --output table

# Check security groups
/usr/local/bin/aws ec2 describe-security-groups \
  --filters "Name=vpc-id,Values=vpc-0eed7af6a5360f7c8" \
  --query 'SecurityGroups[*].[GroupName,GroupId]' \
  --output table
```

### SSH to Nodes
```bash
# SSH to master
ssh -i /home/ec2-user/kafka-spark-keypair.pem ubuntu@10.0.136.17

# SSH to worker1
ssh -i /home/ec2-user/kafka-spark-keypair.pem ubuntu@10.0.137.14

# SSH to worker2
ssh -i /home/ec2-user/kafka-spark-keypair.pem ubuntu@10.0.128.219
```

### Ansible Commands
```bash
# Test connectivity
ansible all -m ping -i inventories/hosts.ini

# Run playbook
ansible-playbook -i inventories/hosts.ini playbooks/site.yml

# Run playbook for specific host
ansible-playbook -i inventories/hosts.ini playbooks/site.yml --limit master

# Run with verbose output
ansible-playbook -i inventories/hosts.ini playbooks/site.yml -vv
```

### Service Management (After Deployment)
```bash
# On master node:
sudo systemctl status zookeeper
sudo systemctl status kafka
sudo systemctl status spark-master

# On worker nodes:
sudo systemctl status spark-worker

# View logs:
sudo journalctl -u zookeeper -f
sudo journalctl -u kafka -f
sudo journalctl -u spark-master -f
```

### Terraform Commands
```bash
cd ~/terraform

# Show current state
terraform show

# List outputs
terraform output

# Update infrastructure
terraform plan
terraform apply

# Destroy infrastructure (WARNING!)
terraform destroy
```

---

## Important Files and Locations

### On Bastion (ec2-user@10.0.13.122)
- SSH Key: `/home/ec2-user/kafka-spark-keypair.pem`
- Terraform: `/home/ec2-user/terraform/`
- Ansible: `/home/ec2-user/ansible/`
- Terraform State: `/home/ec2-user/terraform/terraform.tfstate`

### On Master Node (ubuntu@10.0.136.17) - After Deployment
- Spark: `/opt/spark/`
- Kafka: `/opt/kafka/`
- Zookeeper data: `/var/lib/zookeeper/`
- Kafka logs: `/var/lib/kafka-logs/`
- Spark logs: `/opt/spark/logs/`

### On Worker Nodes (ubuntu@10.0.137.14, ubuntu@10.0.128.219)
- Spark: `/opt/spark/`
- Spark work: `/opt/spark/work/`
- Spark logs: `/opt/spark/logs/`

---

## Network Architecture

```
VPC: vpc-0eed7af6a5360f7c8 (10.0.0.0/16)

Public Subnet: subnet-0a0a5d9cf662dd381 (10.0.0.0/20)
â””â”€â”€ Bastion: 10.0.13.122 (RHEL-based)
    â””â”€â”€ Security Group: sg-0ebcf91b3e3abc4c1

Private Subnet: subnet-0968e7f834411717e (10.0.128.0/20)
â”œâ”€â”€ Master: 10.0.136.17 (Ubuntu)
â”‚   â””â”€â”€ Security Group: sg-052db11f6e080eef4
â”‚   â””â”€â”€ Services: Zookeeper:2181, Kafka:9092, Spark:7077,8080
â”‚
â”œâ”€â”€ Worker1: 10.0.137.14 (Ubuntu)
â”‚   â””â”€â”€ Security Group: sg-09ec1280749c7ce01
â”‚   â””â”€â”€ Services: Spark Worker
â”‚
â””â”€â”€ Worker2: 10.0.128.219 (Ubuntu)
    â””â”€â”€ Security Group: sg-09ec1280749c7ce01
    â””â”€â”€ Services: Spark Worker
```

---

## Troubleshooting

### Issue: Can't SSH to instances
**Solution:** Check security group allows SSH from bastion
```bash
/usr/local/bin/aws ec2 describe-security-groups \
  --group-ids sg-052db11f6e080eef4 \
  --query 'SecurityGroups[0].IpPermissions'
```

### Issue: Ansible can't connect
**Solution:** Verify SSH key path and permissions
```bash
ls -la /home/ec2-user/kafka-spark-keypair.pem
chmod 400 /home/ec2-user/kafka-spark-keypair.pem
```

### Issue: Service not starting
**Solution:** Check logs and status
```bash
sudo systemctl status service-name
sudo journalctl -u service-name -n 50
```

### Issue: Terraform state issues
**Solution:** Check state file
```bash
cd ~/terraform
terraform state list
terraform state show resource-name
```

---

## Cost Optimization

### Current Monthly Cost (24/7 operation):
- Master (t3.medium): ~$30/month
- Workers (2x t3.medium): ~$60/month
- NAT Gateway: ~$32/month
- Data transfer: ~$10/month
- **Total: ~$132/month**

### Stop instances when not in use:
```bash
# Stop all cluster instances
/usr/local/bin/aws ec2 stop-instances \
  --instance-ids i-083657b80007ea86e i-0b3d8475f3c803aed i-0cbd44cdc9eac8c67

# Start instances
/usr/local/bin/aws ec2 start-instances \
  --instance-ids i-083657b80007ea86e i-0b3d8475f3c803aed i-0cbd44cdc9eac8c67
```

---

## Next Steps After Deployment

1. **Verify all services:**
   - SSH to master and check Zookeeper, Kafka, Spark Master
   - SSH to workers and check Spark Workers

2. **Test Kafka:**
   ```bash
   # On master
   /opt/kafka/bin/kafka-topics.sh --create --topic test \
     --bootstrap-server localhost:9092 --partitions 3 --replication-factor 1
   
   /opt/kafka/bin/kafka-topics.sh --list --bootstrap-server localhost:9092
   ```

3. **Test Spark:**
   ```bash
   /opt/spark/bin/spark-submit --master spark://10.0.136.17:7077 \
     --class org.apache.spark.examples.SparkPi \
     /opt/spark/examples/jars/spark-examples_2.12-3.5.0.jar 100
   ```

4. **Deploy your Scala application**

5. **Set up monitoring and logging**

6. **Configure backups for important data**

---

## Documentation Created

1. **SETUP_STEPS.md** (this file) - Complete command history and setup steps
2. **TECHNICAL_DETAILS.md** - Detailed technical explanation of all components

---

**Setup completed on:** November 7, 2025  
**Region:** eu-north-1 (Stockholm)  
**Environment:** Production-ready Spark/Kafka cluster
